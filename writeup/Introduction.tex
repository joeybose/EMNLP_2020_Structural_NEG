\section{Introduction}

Knowledge Graphs represent repositories of information organized as factual triples ($h,r,t$), where head and tail entities are connected via a particular a relation ($r$). Indeed, KGs have seen wide application in a variety of domains from question answering \cite{yao2014information, malinowski2014multi, hao2017end}, natural language processing \cite{berant2013semantic, yu2014improving, yang2019leveraging} to name a few. While often large and expansive, real world KGs such as FreeBase \cite{bollacker2008freebase} and WordNet \cite{miller1995wordnet} are known to be incomplete. Consequently, KG completion via link prediction constitutes a fundamental task \cite{sun2019rotate, kotnis2017analysis, angeli2013philosophers}.

In recent years, there has been a surge of methods employing graph embedding techniques that encode KGs into a lower-dimensional vector space facilitating easier data manipulation \cite{zhang2019nscaching}, while being an attractive framework for handling data sparsity and incompleteness \cite{wang2018incorporating}. To ease computational burdens associated with training KG's embedding models, contrastive estimation \cite{gutmann2010noise} has emerged as the defacto gold standard allowing for the efficient scaling of models to large datasets; it solely trains the model with real data (positive samples) and noise (negative samples) distributions \cite{ma2018noise}. Leveraging contrastive estimation to train KG embedding models thus involves optimizing the model to maximize the score of positive triplets and minimize the score of negative triplets, where score is calculated by the decoder function. %\footnote{Viewing KG embedding models from an encoder-decoder perspective, an encoder maps the nodes into a lower dimensional space while a decoder reconstructs information about a node’s neighborhood from the node’s embedding \cite{hamilton2017representation}.}.

It has been observed that the choice of negative sample distribution plays a crucial role as simple random sampling ---i.e. NCE \cite{gutmann2010noise}, produces negatives that are easily classified and provide little information alongside the observed positive examples \cite{zhang2019nscaching, wang2018incorporating}. Such negative triplets are associated with small scores which impede the optimization process in early stages of training. 
There exists many approaches that seek to learn a more complex negative sampling distribution, such as adversarial samplers \cite{cai2017kbgan, bose2018adversarial}, Self-Adversarial NEG \cite{sun2019rotate}, or even refining the sampling process by using cached high quality negative triplets \cite{zhang2019nscaching}. However, adversarial negative sampling approaches are computationally expensive, while more tractable approaches (e.g., based on caches) tend to ignore the rich graph structure available in KGs. This raises the question of whether we can find a computationally inexpensive negative sampling strategy that can still leverage the rich graph structure of KGs.

%----------------------------------------------------------
%----------------------------------------------------------
% Start of previous versions
\cut{
In recent years, there has been a surge of methods employing graph embedding techniques that encode KGs into a lower-dimensional vector space facilitating easier data manipulation \cite{zhang2019nscaching}, while being an attractive framework for handling data sparsity and incompleteness \cite{wang2018incorporating}. To ease computational burdens associated with training KG's embedding models, negative sampling \CITE has emerged as the defacto gold standard allowing for the efficient scaling of models to large datasets. It has been observed that the choice of (NEG) distribution plays a crucial role as simple random sampling ---i.e. NCE \cite{gutmann2010noise}, produces easy negatives that are easily classified and provide little information alongside the observed positive examples \cite{zhang2019nscaching,wang2018incorporating}. While there exists many approaches that seek to learn a more complex (NEG) distribution, such as adversarial samplers \cite{cai2017kbgan, bose2018adversarial}, Self-Adversarial NEG \cite{sun2019rotate}, or even refining the sampling process by using cached high quality negative triplets \cite{zhang2019nscaching} none explicitly use the rich graph structure directly available in a KG. Can we find a computationally inexpensive NEG distribution that provides high quality hard negatives by directly utilizing graph structure?
}

\cut{
Knowledge Graphs (KGs) are knowledge bases that store and represent information using graph structure, where a relation ($r$) between two entities, namely the head ($h$) and tail ($t$) entities, is denoted as $(h,r,t)$. FreeBase \cite{bollacker2008freebase} and WordNet \cite{miller1995wordnet} are two examples of real-world KGs, which have applications in question answering \cite{yao2014information, malinowski2014multi, hao2017end}, natural language processing \cite{berant2013semantic, yu2014improving, yang2019leveraging}, and many other domains. 

Regardless of containing a significant amount of information, KGs tend to often become incomplete as their domain gets larger, since collecting all possible relations becomes practically infeasible \cite{sun2019rotate,kotnis2017analysis,angeli2013philosophers}. Therefore, knowledge graph completion has turned into a fundamental task when studying KGs, which intends to infer the missing links or the probability of correctness of the existing ones \cite{angeli2013philosophers}. In this regard, recent years have seen a surge of research on graph embedding techniques aiming to encode KGs into a lower-dimensional vector space to facilitate data manipulation \cite{zhang2019nscaching}, and provide a numerical and computational framework for handling data sparsity as well as incompleteness \cite{wang2018incorporating}. 

While training KG embeddings, Negative Sampling (NEG) is an integral part to the learning process as KGs only include positive triples \cite{zhang2019nscaching,trouillon2016complex}. In the literature, early work used uniform sampling to obtain negative triples for training their graph embedding models \cite{wang2017knowledge,bordes2013translating}. While simple, such NEG technique is a fixed scheme and ignores the varying distribution of negative triplets during training, which results in picking negative samples that do not provide any meaningful information \cite{zhang2019nscaching,wang2018incorporating,cai2017kbgan}. Consequently, other techniques were proposed to tackle this issue by choosing negative examples more effectively, some of which involve leveraging Generative Adversarial Networks (GANs) \cite{goodfellow2014generative}, Self-Adversarial NEG \cite{sun2019rotate}, or refining the sampling process by sampling from cached high quality negative triplets \cite{zhang2019nscaching}. 
}

% End of previous versions 
% ----------------------------------------------------------
% ----------------------------------------------------------

\xhdr{Present Work}
In this work, we seek to find hard negative examples to train a KG embedding model by directly using explicit graph structure in the form of an entity's $\emph{k-hop}$ neighborhood. We hypothesize that entities that are within each other's neighbourhood but share no direct relation have higher chances of being related to one another and thus are good candidates for negative sampling. We argue that such local negatives are harder to distinguish and lead to high scores evaluated by the embedding model. We term our approach \emph{Structure Aware Negative Sampling} (SANS), depicted in Fig. \ref{fig:arch}, as it utilizes the underlying graph structure. Our approach, while being algorithmically simple, enjoys inexpensive generation of high quality negatives at inference time. Empirically, we find that negative sampling using SANS consistently leads to higher scores in MRR and Hit@10 over uniform sampling and competitive scores against sophisticated adversarial samplers at a fraction of the computational cost in two representative KG benchmarks and three score functions. 

\cut{
Inspired by GAN-based and cache-based approaches, which all aim to select high-quality negative triplets, in this work, the effect of generating negative samples with respect to nodes' neighbourhood is investigated, hence why our proposed approach is called \emph{Structural Negative Sampling}. The heuristic used while performing sampling is that nodes that are within each other's neighbourhood have higher chances of being related to one another. In this regard, two nodes that are in the same neighbourhood of an arbitrary distance $r$ that are not connected can make a negative triplet that has a high score (evaluated from the embedding model score functions) and is of high quality. Therefore, not only our proposed NEG scheme benefits from using the underlying graph structure, it is also simple and inexpensive to train yet effective. This idea is analogous to how negative triplets are generated in DeepWalk \cite{perozzi2014deepwalk} and node2vec \cite{grover2016node2vec}, in which the node degree or word frequency is considered respectively. A high-level architectural diagram of our proposed NEG scheme is shown in Figure \ref{fig:arch}.
}
\begin{figure}
    \centering
    \includegraphics[width=3in]{writeup/archfig.png}
    \caption{Our proposed approach for training a graph embedding model. In this illustration, $k$ is set to $2$.}
    \label{fig:arch}
\end{figure}

% The rest of this paper is structured as follows. Section \ref{sec:relatedwork} provides an overview of related work. Section \ref{sec:proposedapproach} discusses the proposed structural NEG algorithm. Section \ref{sec:exp} presents the experimental setup and the evaluation protocols used alongside the baselines considered. Section \ref{sec:results} presents the results. Finally, concluding remarks and future directions are given in Section \ref{sec:conclusion}. 


% Section \ref{sec:background} provides background knowledge on different knowledge graph embedding models, and how they are trained with negative samples. 


