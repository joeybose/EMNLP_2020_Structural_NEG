\section{Introduction}

Knowledge Graphs (KG) are repositories of information organized as factual triples ($h,r,t$), where head and tail entities are connected via a particular a relation ($r$). Indeed, KGs have seen wide application in a variety of domains from question answering \cite{yao2014information, malinowski2014multi, hao2017end}, natural language processing \cite{berant2013semantic, yu2014improving, yang2019leveraging} to name a few. While often large, real world KGs such as FreeBase \cite{bollacker2008freebase} and WordNet \cite{miller1995wordnet} are known to be incomplete. Consequently, KG completion via link prediction constitutes a fundamental task \cite{sun2019rotate, kotnis2017analysis, angeli2013philosophers}.

In recent years, there has been a surge of methods employing graph embedding techniques that encode KGs into a lower-dimensional vector space facilitating easier data manipulation \cite{zhang2019nscaching}, while being an attractive framework for handling data sparsity and incompleteness \cite{wang2018incorporating}. To learn such embeddings contrastive learning has emerged as the defacto gold standard. Indeed, contrastive learning approaches enjoy significant computational benefits over methods that require computing an exact softmax over a large candidate set, such as over all possible tail entities when given a head and relation. Another important consideration is modeling need, as certain assumptions are best expressed as some score or energy in margin based or un-normalized probability models \cite{smith2005contrastive}. For example, modeling entity relations as translations or rotations in a vector space naturally leads to a distance-based score to be minimized for observed entity-relation-entity triplets \cite{bordes2013translating}.

Leveraging contrastive estimation to train KG embedding models involves optimizing the model by pushing up energy with respect to observed positive triplets while simultaneously pushing down energy on negative triplets. Consequently, the choice of negative sample distribution plays a crucial role in shaping the energy landscape as simple random sampling ---i.e. NCE \cite{gutmann2010noise}, produces negatives that are easily classified and provide little information alongside in the form of gradient signal. This is easily remedied if the corruption process selects a hard negative example through more complex negative sampling distribution, such as adversarial samplers \cite{cai2017kbgan, bose2018adversarial, sun2019rotate}.\cut{or even refining the sampling process by using cached high quality negative triplets \cite{zhang2019nscaching}.} However, adversarial negative sampling methods are computationally expensive, while more tractable approaches ---i.e. (based on caches \cite{zhang2019nscaching}) are not tailored to the KG setting as they fail to incorporate known graph structure as part of the sampling process. This raises the important question of whether we can obtain a computationally inexpensive negative sampling strategy while benefitting from the rich graph structure of KGs.

\xhdr{Present Work}
In this work, we introduce \emph{Structure Aware Negative Sampling} (SANS), an algorithm which utilizes the graph structure of a KG to find hard negative examples. Specifically, SANS constructs negative samples using a subset of entities restricted to either the head or tail entity's $\emph{k-hop}$ neighborhood. We hypothesize that entities that are within each other's neighbourhood but share no direct relation have higher chances of being related to one another and thus are good candidates for negative sampling. We also experiment with a dynamic sampling scheme based on random walks to approximate a node's local neighborhood. Empirically, we find that negative sampling using SANS consistently leads improves upon uniform sampling and sophisticated GAN-based approaches, and is competitive with SOTA approaches at a fraction of the computational cost and no added parameters.

\cut{
Inspired by GAN-based and cache-based approaches, which all aim to select high-quality negative triplets, in this work, the effect of generating negative samples with respect to nodes' neighbourhood is investigated, hence why our proposed approach is called \emph{Structural Negative Sampling}. The heuristic used while performing sampling is that nodes that are within each other's neighbourhood have higher chances of being related to one another. In this regard, two nodes that are in the same neighbourhood of an arbitrary distance $r$ that are not connected can make a negative triplet that has a high score (evaluated from the embedding model score functions) and is of high quality. Therefore, not only our proposed negative sampling scheme benefits from using the underlying graph structure, it is also simple and inexpensive to train yet effective. This idea is analogous to how negative triplets are generated in DeepWalk \cite{perozzi2014deepwalk} and node2vec \cite{grover2016node2vec}, in which the node degree or word frequency is considered respectively. A high-level architectural diagram of our proposed negative sampling scheme is shown in Figure \ref{fig:arch}.
}
\begin{figure}
    \centering
    \includegraphics[width=3in]{writeup/archfig.png}
    \caption{Our proposed approach for training a graph embedding model. In this illustration, $k$ is set to $2$.}
    \label{fig:arch}
\end{figure}

 


