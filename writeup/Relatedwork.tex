\section{Background and Related Work}
\label{sec:background}
\xhdr{Negative Sampling} 
To address the computational intractability of scaling log-linear models, especially in the context of word and graph embeddings \cite{mikolov2013distributed}, negative sampling can be used. The key idea is to avoid the expensive normalization constant in computing the softmax by training a model to distinguish between observed positive data and fictitious negative examples that are generated by corrupting the positive examples. This general approach is a simplification of the noise contrastive estimation framework \cite{gutmann2010noise}, which is based on Monte-Carlo approximation used in Importance Sampling (IS) \cite{bengio2003quick}, where samples are taken from a distribution that is a mixture of training and noise distributions. 

Given an observed positive triplet $(h,r,t)$ a negative sample can be constructed by corrupting either the head or tail entity to form a new triplet ---i.e. $(h',r,t')$, where either $h',t' \in E$, where $E$ is the set of all entities in the KG.
\cut{
\begin{equation}
    (h',r,t') = \{ (h',r,t)|h'\in E\} \cup \{(h,r,t')|t'\in E \}  \\
\end{equation}
}
Additionally, we assume that the graph embedding models are using a loss function of the following form:
\begin{align}
    \Lb = &\sum_{(h,r,t) \in S} -\text{log }\sigma(\gamma - d_{r}(\boldsymbol{h},\boldsymbol{t}))  \notag \\ 
    - &\sum_{i=1}^{k} \frac{1}{k}\text{log }\sigma(d_r(\boldsymbol{h}_i^{'},\boldsymbol{t}_i^{'})-\gamma)
\end{align}
where $d_r(h,t)$ denotes the score assigned to the compatibility of head and tail entities under the relation $r$, $\gamma$ is a fixed margin, $\sigma$ is the sigmoid function, and $k$ is the number of negative samples.

% ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
\xhdr{Non-Fixed Negative Sampling}
\label{sec:relatedwork}
As proposed in \cite{mikolov2013distributed}, negative triplets can be generated using a uniform sampling scheme. However, such uniform and fixed sampling scheme results in easily-classified negative triplets during training, which do not provide any meaningful information \cite{sun2019rotate, zhang2019nscaching}. Consequently, as the training progresses, most of the sampled negative triplets receive small scores and almost zero gradient, impeding the training of the graph embedding model after only a small number of iterations. 

To address the issue of easy negatives, ~\citet{sun2019rotate} propose Self-Adversarial negative sampling, which weighs each sampled negative according its probability under the embedding model. Alternatively, the authors in \cite{wang2018incorporating} and \cite{cai2017kbgan} try creating high quality negative samples by exploiting Generative Adversarial Networks (GANs) \cite{goodfellow2014generative}. Although effective, GAN-based sampling schemes are expensive to train and require gradient estimation techniques, ---i.e. REINFORCE \cite{williams1992simple} , whose variance can scale linearly with the size of $E$ \cite{cai2017kbgan}. Another, elegant approach to hard negative mining involves using a cache of high-quality negative triplets ---i.e. those with high scores \cite{zhang2019nscaching}. In comparison to the GAN-based methods, NSCaching uses considerably less parameters and is easier to train. 

 

\cut{
To overcome the limitations of GAN-based negative sampling methods, ~\citet{zhang2019nscaching} propose a negative sampling approach based on a cache, named NSCaching using which a set of high-quality negative triplets (those with high scores) is maintained and negative triples are sampled from. In comparison to the GAN-based methods, NSCaching uses considerably less parameters and is easier to train. }



