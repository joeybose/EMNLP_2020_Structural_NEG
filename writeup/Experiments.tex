\section{Experiments}
\label{sec:results}

\input{writeup/t_uni_and_sota}
\input{writeup/t_adv_results}

We investigate the application of SANS based negatives to train KG's embedding models based on  the TransE, DistMult, and RotatE models for the task of KG completion. We evaluate our proposed approach on standard benchmarks consisting of a variant of FreeBase in FB15K-237 \cite{bollacker2008freebase}, WN18 and WN18RR \cite{miller1995wordnet}. From our experiments we seek to answer the following questions: 
\begin{itemize}
    \item[\textbf{(Q1)}] \textbf{Hard Negatives: Can we sample hard negatives purely using graph structure?} 
    \item[\textbf{(Q2)}] \textbf{Can we combine graph structure with other SOTA negative samplers ?} 
    \item[\textbf{(Q3)}] \textbf{Can we effectively approximate the adjacency tensor with random walks} 
\end{itemize}
Throughout our experiments we rely on three representative baselines, namely uniform negative sampling \cite{bordes2013translating}, KBGAN \cite{cai2017kbgan}, and NSCaching \cite{zhang2019nscaching}. We also compare with the current SOTA approach in Self-Adversarial negative sampling \cite{sun2019rotate} and we test whether local graph structure can also be leveraged in this setting. \cut{As evaluation measures we use Hits at N (H@N), and Mean Reciprocal Rank (MRR).}

\cut{
\subsection{Experimental Setting}
% Dataset and evaluation metrics

% \input{writeup/t_uniform_results}
\input{writeup/t_uni_and_sota}
\input{writeup/t_adv_results}
% \input{writeup/t_sota_results}

To conduct experiments for our proposed methods, datasets FB15K-237, which is a variant of FB15K \cite{bollacker2008freebase}, alongside WN18 and WN18RR \cite{miller1995wordnet} are used. For evaluation, our results will be achieved by training the TransE, DistMult, and RotatE models, during which the train set triplets consist of positive triplets $(h , r, t)$ as well as negative triplets, $(h', r, t')$. To evaluate the impact of our negative sampling approaches on the learned embedding, Hits at N (H@N), and Mean Reciprocal Rank (MRR) are used as evaluation measures. Lastly, the baselines in our experiments are the performance of uniform and Self-Adversarial negative sampling \cite{sun2019rotate} in effectively training the selected graph embedding models.

% Hyperparameter and baseline

%%%%% note: Random Walks needs to be capitalized when it's used as an alg. name.

The important hyperparameters in our study are $k$, which determines the \emph{k-hop} neighbourhood of each entity, and the number of random walks, $\omega$, when \emph{Random Walks} is being used to approximate the \emph{k-hop} neighbourhood. The impact of different values of these hyperparameters are thoroughly discussed in Section \ref{sec:supplemental}.  Additionally, the graph embedding models' hyperparameters that yield their best performance on the validation set can be found in Section \ref{sec:supplemental}.
}
\subsection{Results}
We now address the core experimental questions (\textbf{Q1}-\textbf{Q3}), highlighting the performance of SANS.

\xhdr{Q1:} Table \ref{tab:unisota_comp} consists of our main results when restricting the set of negative samples to the \emph{k-hop} neighbourhood. We find that SANS negatives almost always lead to harder negative samples over Uniform and KBGAN negatives on all three datasets. \cut{This is implied in the consistent superior performance of Uniform SANS in FB15K-237, while exhibiting comparable performance to it if not outperforming it fully in other settings.}Furthermore, SANS achieves highly comparable performance with NSCaching, a method that utilizes external memory to cache high-quality negatives, with an average \red{\%XX} difference in MRR, thus confirming our hypothesis that graph structure indeed helps in hard negative mining.
\cut{
Lastly, by acknowledging the similar performance of Self-Adversarial and Self-Adversarial SANS in Table \ref{tab:comp_adv}, the importance of local negatives when training graph embedding models is more pronounced. 
}
\xhdr{Q2:} We now combine our approach SANS with Self-Adversarial negative sampling \cite{sun2019rotate}. Our results are presented in Table \ref{tab:comp_adv} under Self-Adv. SANS which reweighs, using the probability of observing the triplet under embedding models, negative samples which are constrained to the \emph{k-hop} neighborhood. We observe comparable performance between the two approaches, but crucially this is achieved by considering less than $1\%$ of the entities in datasets like WN18 and WN18RR (see Table 6 in Appendix). It is indeed a remarkable fact that such a small set of entities localized to a given nodes neighborhood are needed to achieve near SOTA performance further highlighting the importance of graph structure.
\cut{
thus highlights its potential to be combined with the state of the art negative sampling methods. Additionally, accounting for graph structure enhances their computational feasibility, for resulting in an adjacency tensor that is more sparse due to only considering the \emph{k-hop} neighbourhood of entities within the KG when choosing negatives. 
}

\xhdr{Q3:} We now analyze the impact of approximating the local neighborhood using random walks. Figure \ref{fig:ablation} illustrates the effect of varying the number of random walks $\omega$ with the approximation of neighborhoods of different radii. We report two baselines, one being the performance of uniform sampling, and the other being our best performance achieved by Uniform SANS when combined with TransE, for which the k-hop tensor was explicitly computed. Interestingly, we find that the k-hop tensor can not only be well approximated with 3000 random walks but also outperforms both baselines in MRR. We reconcile this result by noting that certain nodes have a higher probability of being sampled due to sharing a larger number of paths with the centre node, resulting in a weighted negative sampling scheme.

\begin{figure}[h]
    \centering
    \includegraphics[width=2.5in]{writeup/Results/Ablation.png}
    \caption{The effect of the number of random walks
     for Uniform SANS with the TransE on FB 15k-237.}
    \label{fig:ablation}
\end{figure}

% END OF RESULTS
% ---------------------------------------------------------------

\cut{
Table \ref{tab:unisota_comp} compares the performance of uniform SANS negative sampling with state of the art algorithms, used to train different graph embedding models on different datasets. As indicated, not only uniform SANS beats uniform negative sampling in FB15K-237, it almost always outperforms it in the WN18 dataset, only losing to it marginally.  This out-performance is especially significant when uniform SANS is fused with TransE and DistMult, where an increase of 0.2143 and 0.2864 in MRR is noticed respectively. Comparing Uniform SANS with other negative sampling approaches, SANS beats KBGAN in all experimental setups except for one. Additionally, it beats NSCaching when used to train TransE. This finding re-iterates the ``hardness'' of negative examples that lie in the \emph{k-hop} neighbourhood of the nodes, as NSCaching works by caching and sampling from high quality negative triplets.} 

\cut{
On the other hand, Table \ref{tab:comp_adv} is indicative of the performance of Self-Adversarial negative sampling against our Self-Adversarial SANS. As noted, Self-Adversarial SANS beats Self-Adversarial on FB15K-237 and WN18 when fused with DisMult, while achieves close performance to it in other settings. By acknowledging the similarity in their performance and the nature of the distribution of negative samples used in Self-Adversarial SANS, the importance of local negatives when training graph embedding models is emphasized, supporting our hypothesis. This observation further implies the computational feasibility of our approach compared to Self-Adversarial, for using a smaller distribution of negative triplets. \cut{, as listed in Table \ref{tab:percentage}.}}

\cut{Lastly, Table \ref{tab:comp_sota} compares the performance of uniform SANS with some of the state-of-the-arts. As seen, uniform SANS beats KBGAN in all experimental setups except for one. Additionally, it beats NSCaching when used to train TransE. This finding re-iterates the ``hardness'' of negative examples that lie in the \emph{k-hop} neighbourhood of the nodes, as NSCaching works by caching and sampling from high quality negative triplets.} 

\cut{To infer the number of random walks which can adequately approximate the k-hop tensor, we investigated the effect of different numbers of random walks, $\omega$, ranging from 100 to 3000 illustrated in Figure \ref{fig:ablation}. In these graphs, two baselines are exhibited, namely the one representing the output when negative sampling was done uniformly, and the other being our best performance achieved by our Uniform SANS combined with TransE, for which the k-hop tensor was explicitly computed. According to our observations, our model beats both baselines when the k-hop tensor is approximated with 3000 random walks. This result can be justified by acknowledging that the nodesâ€™ indices that are within close proximity in the k-hop tensor will have larger values; this will yield a weighted negative sampling scheme and may be the contributor to the increased performance.}







